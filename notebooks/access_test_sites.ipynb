{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Accessibility Analysis *At Scale*: \n",
    "## Measuring Access to Covid Testing Sites in California"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook calculates access to Covid-19 testing sites for every block group in California. Apart from the locations of test sites, all data are collected on the fly. To account for the massive size of the study area (all of CA) the analysis is chunked at the county level; access is computed for the blockgroups in each county (with both supply and demand data buffered 8km beyond the county borders to mitigate edge effects), then the counties are recombined into a statewide set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cenpy\n",
    "import osmnet\n",
    "import os\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandana as pdna\n",
    "import urbanaccess as ua\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from geosnap import Community, datasets\n",
    "from access import access as Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextily as ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from access import weights as acweights\n",
    "from healthacc.travel_matrix import compute_travel_cost_adjlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segregation.util import project_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Locations (testing supply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_locations = gpd.read_file(\"../data/accessbility/Testing_Locations_7_15.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_locations['count'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census Data (testing demand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = datasets.counties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.read_excel('../data/all-geocodes-v2017.xlsx', skiprows=4, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names['geoid'] = names['State Code (FIPS)'] + names['County Code (FIPS)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = counties.merge(names, on='geoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_counties = counties[counties.geoid.str.startswith('06')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_counties.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_blkgrps= gpd.read_file(\"../data/accessbility/CA_BG_Income.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_blkgrps.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_blkgrps = ca_blkgrps.to_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = [county[-3:] for county in ca_counties.geoid.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our blockgroup geometries seem to contain income data, but we also need population data to pass to `access`. We'll grab blockgroup-level population data using `cenpy`, but since blockgroups [arent implemented]() in the products api yet, we need to loop over each county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../data/ca_blockgroup_population.csv'):\n",
    "\n",
    "    cadict = []\n",
    "    conn = cenpy.products.APIConnection(\"ACSDT5Y2017\")   \n",
    "    for county in counties:\n",
    "        data = conn.query([\"B01001_001E\"], geo_unit = 'block group', geo_filter = {\"state\": \"06\", \"county\": f'{county}'})\n",
    "        cadict.append(data)\n",
    "    gdf = pd.concat(cadict)\n",
    "    gdf.to_csv('../data/ca_blockgroup_population.csv')\n",
    "else:\n",
    "    gdf = pd.read_csv('../data/ca_blockgroup_population.csv', converters={'state':str, 'county':str, 'tract':str, 'block group': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['geoid'] = gdf.state + gdf.county + gdf.tract + gdf['block group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockgroups = ca_blkgrps.merge(gdf, left_on='GEOID10', right_on='geoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockgroups.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSM Data (pedestrian network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the networks for each county we need to do a little bit of laborious data processing. The study area of *all* of california is too large of a problem size to deal with in a single go (we're likely talking  about a point-to-point travel matrix of billions of OSM intersections...) so we split the data and process by county. To mitigate edge effects though, we need to buffer each county beyond the travel threshold we care about and operate on the buffered data before clipping it back to the proper spatial extent. OSM handles data in lat/long, so we first need to iterate over each california county and project it into the proper CRS. California is a big state, so it falls into two different UTM zones (10 and 11). I don't want to lookup the proper CRS for each county, so here we use the `utm` library to:\n",
    "- loop over each county\n",
    "- project it to utm\n",
    "- buffer it out 8000m\n",
    "- project it back to WGS and collect the bounding box\n",
    "- send the bbox to OSM to download the network and store it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_utm(polygon):\n",
    "    \n",
    "    lon = polygon.centroid.x\n",
    "    lat = polygon.centroid.y\n",
    "    \n",
    "    zone = utm.from_latlon(lat,lon)\n",
    "    crs = crs = f\"+proj=utm +zone={str(zone[2])} +ellps=WGS84 +datum=WGS84 +units=m +no_defs\"\n",
    "    \n",
    "    return crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer_bbox(polygon):\n",
    "    \n",
    "    crs = determine_utm(polygon)\n",
    "    gdf = gpd.GeoDataFrame(pd.Series([polygon], name='geometry'))\n",
    "    gdf.crs=4326\n",
    "    gdf = gdf.to_crs(crs).buffer(8000)\n",
    "    gdf = gdf.to_crs(4326)\n",
    "    \n",
    "    return gdf.total_bounds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for county in ca_counties.iterrows():\n",
    "    fname = f\"../data/networks/osmnet_{county[1].geoid}.h5\"\n",
    "    if not os.path.exists(fname):\n",
    "        bbox = buffer_bbox(county[1].geometry)\n",
    "        nodes, edges = osmnet.network_from_bbox(bbox=tuple(bbox))\n",
    "        net = pdna.Network(nodes[\"x\"],\n",
    "                           nodes[\"y\"],\n",
    "                           edges[\"from\"],\n",
    "                           edges[\"to\"],\n",
    "                           edges[[\"distance\"]])\n",
    "        net.save_hdf5(fname)\n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GTFS Data (transit network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my first attempt, I used the builtin downloader but the results were unsatisfactory, so I wrote a couple functions to hit the transitland api instead, which works better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~Brute force: add all feeds for california*, then clip the network to an extent using the buffered bboox for each county. GTFS data are pretty small so this is easy and fast~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~two things to note here. First, urbanaccess is setup to query from GTFS Data Exchange which was shut down a few years ago, so the data may be stale. Second, we're doing a string search for California, which may not necessarily be in every CA transit feed name. TransitLand has popped up to replace  GTGS Data Exchange, and its api allows searching by geograpic extent via bounding boxes. There's an experimental (but stale) branch of ua that implements a transitland search here https://github.com/UDST/urbanaccess/blob/kuanb-gtfs-feeds-query-util/urbanaccess/gtfsfeeds.py but i havent tested it. ~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from healthacc import feeds_from_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these throw a 403. I think the call to `requests` inside urbanaccess needs to be beefed up with user agent and headers and such \n",
    "# <https://stackoverflow.com/questions/38489386/python-requests-403-forbidden>\n",
    "fourofours = ['metro', 'laketahoe','southwestpoint','metrolinktrains', 'riversidetransitagency','blue','tidelinewatertaxi','countyconnection']\n",
    "\n",
    "# throw  401 for not having an API key for 511.org\n",
    "fouroones = ['capitolcorridor','angelislandtiburonferry', 'acealtamontcorridorexpress', 'goldengateferry', 'alcatrazhornblowerferry', 'smart', \\\n",
    "            'unioncitytransit', 'vacavillecitycoach', 'vinenapacounty', 'sanfranciscobayferry', 'americancanyontransit', 'dumbartonexpress']\n",
    "#this one times out\n",
    "tout = ['trideltatransit']\n",
    "# these are corrupt zips or bad GTFS data\n",
    "# usually missing calendar_dates, but other errors too\n",
    "incorrect = ['commuteorgshuttle', 'culvercitybus', 'airportexpressinc', 'madera','anaheim','vctc','goldengatetransit','kerncounty','delnorte',\\\n",
    "            'yosemite','longbeachtransit', 'getbus', 'avalon', 'amtrak', 'petalumatransit'] \n",
    "\n",
    "feederrors = fourofours + fouroones + tout + incorrect\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(county, bbox):\n",
    "    COUNTY_PATH = f\"../data/counties/{county}\"\n",
    "    if os.path.exists(f\"../data/matrices/{county}_adj.parquet\"):\n",
    "        print(f\"network adj list for {county} already exists\")\n",
    "        combined_net=''\n",
    "    else:\n",
    "        osm_network = pdna.Network.from_hdf5(f\"../data/networks/osmnet_{county}.h5\")\n",
    "\n",
    "        try:\n",
    "            # download gtfs data if we need it\n",
    "            if os.path.exists(f\"{COUNTY_PATH}/{county}_gtfs.h5\"):\n",
    "                loaded_feeds = ua.gtfs.network.load_processed_gtfs_data(f\"{county}_gtfs.h5\", dir=COUNTY_PATH,)\n",
    "            else:   \n",
    "                # get GTFS feeds for the area\n",
    "                feeds = feeds_from_bbox(bbox)\n",
    "                for feed in list(feeds.keys()):\n",
    "                    if feed in feederrors:\n",
    "                        feeds.pop(feed)\n",
    "                if len(ua.gtfsfeeds.feeds.to_dict()['gtfs_feeds'])>0:\n",
    "                    ua.gtfsfeeds.feeds.remove_feed(remove_all=True)  #feeds object is global so reset it from last iter\n",
    "                ua.gtfsfeeds.feeds.add_feed(feeds)\n",
    "                ua.gtfsfeeds.download()\n",
    "                loaded_feeds = ua.gtfs.load.gtfsfeed_to_df(f\"{COUNTY_PATH}/gtfsfeed_text/\",\n",
    "                                                           bbox=bbox,\n",
    "                                                           remove_stops_outsidebbox=True)\n",
    "                ua_to_h5(loaded_feeds, f\"{COUNTY_PATH}/{county}_gtfs.h5\")\n",
    "\n",
    "            # Create transit and OSM networks and combine them into a single multimodal net\n",
    "            ua.create_transit_net(gtfsfeeds_dfs=loaded_feeds,\n",
    "                                           day='monday',\n",
    "                                           timerange=['07:00:00', '10:00:00'],\n",
    "                                           calendar_dates_lookup=None)\n",
    "            osm_network.nodes_df['id'] = osm_network.nodes_df.index\n",
    "            ua_osm = ua.create_osm_net(osm_edges=osm_network.edges_df,\n",
    "                                      osm_nodes=osm_network.nodes_df,\n",
    "                                      travel_speed_mph=3)\n",
    "            urbanaccess_net = ua.ua_network\n",
    "            ua.integrate_network(urbanaccess_network=urbanaccess_net,\n",
    "                                 headways=False)\n",
    "            combined_net = pdna.Network(urbanaccess_net.net_nodes[\"x\"],\n",
    "                                        urbanaccess_net.net_nodes[\"y\"],\n",
    "                                        urbanaccess_net.net_edges[\"from_int\"],\n",
    "                                        urbanaccess_net.net_edges[\"to_int\"],\n",
    "                                        urbanaccess_net.net_edges[[\"weight\"]])\n",
    "\n",
    "        except KeyError:  # fallback to ped-only network if no gtfs in that county\n",
    "            combined_net = pdna.Network(osm_network.nodes_df[\"x\"],\n",
    "                                        osm_network.nodes_df[\"y\"],\n",
    "                                        osm_network.edges_df[\"from\"],\n",
    "                                        osm_network.edges_df[\"to\"],\n",
    "                                        osm_network.edges_df[[\"weight\"]])\n",
    "    return combined_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ua_to_h5(loaded_feeds, path):\n",
    "    \n",
    "    hdf = pd.HDFStore(path)\n",
    "    hdf['calendar'] =loaded_feeds.calendar\n",
    "    hdf['calendar_dates'] =loaded_feeds.calendar_dates\n",
    "    hdf['headways'] =loaded_feeds.headways\n",
    "    hdf['routes'] =loaded_feeds.routes\n",
    "    hdf['stop_times'] =loaded_feeds.stop_times\n",
    "    hdf['stop_times_int'] =loaded_feeds.stops\n",
    "    hdf['stops'] =loaded_feeds.stops\n",
    "    hdf['trips'] = loaded_feeds.trips\n",
    "    hdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Accessibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(county):\n",
    "    \n",
    "    df = ca_blkgrps[ca_blkgrps.GEOID10.str[:5]==county].copy()\n",
    "    boundary = project_gdf(df).buffer(8000)\n",
    "    utm = boundary.crs\n",
    "    boundary = boundary.to_crs(4326).unary_union\n",
    "    \n",
    "    # this part is expensive\n",
    "    buf_testint = test_locations[test_locations.intersects(boundary)]    \n",
    "    buf_blkgrps = ca_blkgrps[ca_blkgrps.centroid.intersects(boundary)]\n",
    "\n",
    "    geoms = buf_blkgrps[['GEOID10', 'geometry']]\n",
    "    \n",
    "    \n",
    "    dests = gpd.sjoin(geoms, buf_testint, how='left', op='intersects').groupby('GEOID10').sum().merge(geoms, on='GEOID10')\n",
    "    \n",
    "    buf_blkgrps = buf_blkgrps.merge(dests[['GEOID10', 'count']], on='GEOID10')\n",
    "    buf_blkgrps = buf_blkgrps.merge(blockgroups[['GEOID10', 'B01001_001E']], on='GEOID10', how='left')\n",
    "    buf_blkgrps =  gpd.GeoDataFrame(buf_blkgrps)\n",
    "    buf_blkgrps.crs = 4326\n",
    "\n",
    "    \n",
    "    return buf_blkgrps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_access(county, blkgrps, network):\n",
    "    if os.path.exists(f\"../data/matrices/{county}_adj.parquet\"):\n",
    "        adjlist = pd.read_parquet(f\"../data/matrices/{county}_adj.parquet\")\n",
    "    else:\n",
    "        adjlist = compute_travel_cost_adjlist(blkgrps, blkgrps, network, reindex_dest='GEOID10', reindex_orig='GEOID10')\n",
    "        adjlist.assign(temp=adjlist.origin+adjlist.destination).drop_duplicates(subset='temp', inplace=True)\n",
    "        adjlist.to_parquet(f\"../data/matrices/{county}_adj.parquet\")\n",
    "    ac_test=Access(demand_df = blkgrps,\n",
    "            demand_index = 'GEOID10',\n",
    "            demand_value = 'B01001_001E',\n",
    "            supply_df    = blkgrps,\n",
    "            supply_index = 'GEOID10',\n",
    "            cost_df=adjlist.replace(0.0,1.0),  # gravity chokes if travel time is 0\n",
    "            cost_origin='origin',\n",
    "            cost_dest='destination',\n",
    "            cost_name='cost',\n",
    "            supply_value='count',\n",
    "            neighbor_cost_df     = adjlist.replace(0.0,1.0),\n",
    "            neighbor_cost_origin = 'origin',\n",
    "            neighbor_cost_dest   = 'destination',\n",
    "            neighbor_cost_name   = 'cost'\n",
    "      )\n",
    "    try: \n",
    "        ac_test.raam(name = \"raam\", tau = 60);\n",
    "        ac_test.access_df[\"raam_count\"] = 1 / ac_test.access_df[\"raam_count\"] \n",
    "    except:\n",
    "        print('raam failed')\n",
    "    ac_test.two_stage_fca(name =\"2sfca\", max_cost = 60,)\n",
    "    ac_test.enhanced_two_stage_fca(name = \"g2sfca\", weight_fn = gaussian)\n",
    "    ac_test.three_stage_fca(name = \"3sfca\")\n",
    "    ac_test.weighted_catchment(name = \"catch_gravity\", weight_fn = gravity)\n",
    "    ac_test.weighted_catchment(name = \"catch_gaussian\", weight_fn = gaussian)\n",
    "    ac_test.fca_ratio(name = \"fca60\",      max_cost = 30)\n",
    "    ac_test.fca_ratio(name = \"fca120\",      max_cost = 60) \n",
    "    \n",
    "    return ac_test.access_df[ac_test.access_df.index.str.startswith(county)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urbanaccess.config import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ua.gtfsfeeds.feeds.to_dict()['gtfs_feeds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from access import weights as acweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gravity = acweights.gravity(scale = 60, alpha = -1)\n",
    "gaussian = acweights.gaussian(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings.log_console=False # turn off urbanccess verbosity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell does all the computation and it will take awhile to run. Despite pandana's impressive speed, this is a lot of data to crunch. Most counties are done in a minute or two, but LA takes about an hour on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_statewide = []\n",
    "for index, row in ca_counties.iterrows():\n",
    "    try:\n",
    "        if not os.path.exists(f\"../data/counties/{row.geoid}_access.parquet\"):\n",
    "\n",
    "            gdf = prepare_data(row.geoid)\n",
    "            network = build_network(row.geoid, gdf.total_bounds)\n",
    "            access = compute_access(row.geoid, gdf.drop_duplicates(subset=['GEOID10']), network)\n",
    "            access.to_parquet(f\"../data/counties/{row.geoid}_access.parquet\")\n",
    "        else:\n",
    "            access = pd.read_parquet(f\"../data/counties/{row.geoid}_access.parquet\")\n",
    "        ca_statewide.append(access)\n",
    "    except:\n",
    "        print(f\"{row.geoid} failed\")\n",
    "statewide_access = pd.concat(ca_statewide)\n",
    "statewide_access.to_parquet(\"../data/ca_statewide_access.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statewide_access = pd.read_parquet('../data/ca_statewide_access.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockgroups = blockgroups.merge(statewide_access, left_on='GEOID10', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2,figsize=(15,10))\n",
    "blockgroups.plot('g2sfca_count', cmap='blues', scheme='quantiles', k=8, ax=axs[0])\n",
    "blockgroups[blockgroups.GEOID10.str.startswith('06073')].plot('g2sfca_count', cmap='blues', scheme='quantiles', k=8, ax=axs[1])\n",
    "for ax in axs:\n",
    "    ax.axis('off')\n",
    "fig.suptitle('Access to COVID Test Sites', fontsize=30)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/ca_example.png', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures = ['raam_count', '2sfca_count', 'g2sfca_count', '3sfca_count', 'catch_gravity_count', 'catch_gaussian_count', 'fca60_count', 'fca120_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockgroups = blockgroups.to_crs(3857)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, county in ca_counties.iterrows():\n",
    "    fig, ax = plt.subplots(2,4, figsize=(24,16))\n",
    "    axs=ax.flatten()\n",
    "    df = blockgroups[blockgroups.GEOID10.str.startswith(f\"{county.geoid}\")]\n",
    "    for i, col in enumerate(measures):\n",
    "        try:\n",
    "            df.plot(col, cmap='Blues', scheme='quantiles', k=8, ax=axs[i], alpha=0.6)\n",
    "            ctx.add_basemap(ax=axs[i], source=ctx.providers.Stamen.TonerLite)\n",
    "            axs[i].axis('off')\n",
    "            axs[i].set_title(f'{col[:-6]}')\n",
    "            fig.suptitle(f\"{county['Area Name (including legal/statistical area description)']}\", fontsize=30)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'../figures/counties/{county.geoid}.png', dpi=200)\n",
    "        except:\n",
    "            #raise\n",
    "            print(f\"{col} failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:healthacc]",
   "language": "python",
   "name": "conda-env-healthacc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
